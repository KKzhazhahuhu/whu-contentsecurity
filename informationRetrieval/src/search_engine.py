# ===== src/search_engine.py (æœ€ç»ˆå·¥ä½œç‰ˆæœ¬) =====
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Set
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
import logging
from collections import defaultdict, Counter
import sys
import os
from pathlib import Path

# ä¿®å¤ï¼šæ›´è¯¦ç»†çš„å¯¼å…¥æ£€æµ‹
SEMANTIC_SEARCH_AVAILABLE = False
SentenceTransformer = None
Transformer = None
Pooling = None

try:
    from sentence_transformers import SentenceTransformer
    from sentence_transformers.models import Transformer, Pooling

    SEMANTIC_SEARCH_AVAILABLE = True
    print("âœ… sentence-transformers å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸  sentence-transformers å¯¼å…¥å¤±è´¥: {e}")
    SEMANTIC_SEARCH_AVAILABLE = False

sys.path.append(str(Path(__file__).parent.parent))
import config


class SemanticSearchEngine:
    """è¯­ä¹‰æœç´¢å¼•æ“"""

    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.model = None
        self.semantic_embeddings = None
        self.logger = logging.getLogger(__name__)

        if SEMANTIC_SEARCH_AVAILABLE and SentenceTransformer is not None:
            try:
                # æ£€æŸ¥æœ¬åœ°æ¨¡å‹
                local_model_path = Path(__file__).parent.parent / "models" / model_name

                if local_model_path.exists() and local_model_path.is_dir():
                    # æ£€æŸ¥å¿…è¦çš„æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    required_files = ['config.json', 'pytorch_model.bin', 'tokenizer.json']
                    missing_files = [f for f in required_files if not (local_model_path / f).exists()]

                    if not missing_files:
                        print(f"å‘ç°å®Œæ•´çš„æœ¬åœ°æ¨¡å‹: {local_model_path}")

                        # ä½¿ç”¨æ‰‹åŠ¨æ„å»ºæ–¹æ³•ï¼ˆå·²éªŒè¯æˆåŠŸï¼‰
                        success = False

                        try:
                            print("ä½¿ç”¨æ‰‹åŠ¨æ„å»ºæ–¹æ³•åŠ è½½æ¨¡å‹...")

                            # æ‰‹åŠ¨åˆ›å»º Transformer æ¨¡å‹
                            transformer = Transformer(str(local_model_path))
                            print("âœ… Transformer ç»„ä»¶åŠ è½½æˆåŠŸ")

                            # åˆ›å»º Pooling å±‚ï¼ˆä½¿ç”¨ mean poolingï¼‰
                            pooling = Pooling(
                                transformer.get_word_embedding_dimension(),
                                pooling_mode='mean'
                            )
                            print("âœ… Pooling ç»„ä»¶åˆ›å»ºæˆåŠŸ")

                            # ç»„åˆæˆ SentenceTransformer
                            self.model = SentenceTransformer(modules=[transformer, pooling])
                            print(f"âœ… æ‰‹åŠ¨æ„å»ºè¯­ä¹‰æœç´¢æ¨¡å‹æˆåŠŸ: {model_name}")
                            success = True

                        except Exception as e1:
                            print(f"âŒ æ‰‹åŠ¨æ„å»ºå¤±è´¥: {e1}")

                        if not success:
                            print("âŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥")
                            raise Exception("æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥")

                    else:
                        print(f"âŒ æœ¬åœ°æ¨¡å‹ä¸å®Œæ•´ï¼Œç¼ºå°‘æ–‡ä»¶: {missing_files}")
                        raise Exception("æœ¬åœ°æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´")
                else:
                    print(f"æœ¬åœ°æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {local_model_path}")
                    raise Exception("æœ¬åœ°æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")

                self.logger.info(f"è¯­ä¹‰æœç´¢æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")

            except Exception as e:
                self.logger.warning(f"æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print(f"âŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print("ğŸ’¡ å°è¯•åœ¨çº¿åŠ è½½...")

                # å°è¯•åœ¨çº¿åŠ è½½
                try:
                    print(f"å°è¯•åœ¨çº¿åŠ è½½æ¨¡å‹: {model_name}")
                    self.model = SentenceTransformer(model_name)
                    print(f"âœ… åœ¨çº¿è¯­ä¹‰æœç´¢æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
                except Exception as e2:
                    print(f"âŒ åœ¨çº¿åŠ è½½ä¹Ÿå¤±è´¥: {e2}")
                    print("ğŸ’¡ ç³»ç»Ÿå°†ä½¿ç”¨å…³é”®è¯æœç´¢æ¨¡å¼")
                    self.model = None
        else:
            self.logger.warning("sentence-transformersä¸å¯ç”¨ï¼Œè¯­ä¹‰æœç´¢åŠŸèƒ½ä¸å¯ç”¨")
            print("âš ï¸  sentence-transformersä¸å¯ç”¨ï¼Œè¯­ä¹‰æœç´¢åŠŸèƒ½ä¸å¯ç”¨")

    def build_embeddings(self, documents: List[str]):
        """æ„å»ºæ–‡æ¡£çš„è¯­ä¹‰åµŒå…¥"""
        if not self.model:
            print("âŒ è¯­ä¹‰æ¨¡å‹æœªåŠ è½½ï¼Œè·³è¿‡åµŒå…¥æ„å»º")
            return None

        try:
            self.logger.info("å¼€å§‹æ„å»ºè¯­ä¹‰åµŒå…¥...")
            print("ğŸ”„ å¼€å§‹æ„å»ºè¯­ä¹‰åµŒå…¥...")

            # è¿‡æ»¤ç©ºæ–‡æ¡£
            valid_docs = [doc if doc and doc.strip() else "empty document" for doc in documents]
            print(f"   å¤„ç†æ–‡æ¡£æ•°é‡: {len(valid_docs)}")

            # æ‰¹é‡ç¼–ç ï¼Œä½¿ç”¨è¾ƒå°çš„batch_sizeé¿å…å†…å­˜é—®é¢˜
            self.semantic_embeddings = self.model.encode(
                valid_docs,
                batch_size=16,  # å‡å°batch_size
                show_progress_bar=True,
                convert_to_numpy=True,
                normalize_embeddings=True  # æ ‡å‡†åŒ–åµŒå…¥
            )

            self.logger.info(f"è¯­ä¹‰åµŒå…¥æ„å»ºå®Œæˆ: {self.semantic_embeddings.shape}")
            print(f"âœ… è¯­ä¹‰åµŒå…¥æ„å»ºå®Œæˆ: {self.semantic_embeddings.shape}")
            return self.semantic_embeddings
        except Exception as e:
            self.logger.error(f"æ„å»ºè¯­ä¹‰åµŒå…¥å¤±è´¥: {e}")
            print(f"âŒ æ„å»ºè¯­ä¹‰åµŒå…¥å¤±è´¥: {e}")
            return None

    def search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        """è¯­ä¹‰æœç´¢"""
        if not self.model or self.semantic_embeddings is None:
            return []

        try:
            # ç¼–ç æŸ¥è¯¢
            query_embedding = self.model.encode([query], convert_to_numpy=True, normalize_embeddings=True)

            # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
            similarities = cosine_similarity(query_embedding, self.semantic_embeddings).flatten()

            # è·å–top-kç»“æœ
            top_indices = np.argsort(similarities)[::-1][:top_k]

            return [(int(idx), float(similarities[idx])) for idx in top_indices
                    if similarities[idx] > 0.1]  # è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
        except Exception as e:
            self.logger.error(f"è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return []


class SpellCorrector:
    """æ‹¼å†™çº é”™å™¨"""

    def __init__(self, max_distance: int = 2):
        self.max_distance = max_distance
        self.word_freq = Counter()
        self.vocabulary = set()

    def build_vocabulary(self, documents: List[str]):
        """ä»æ–‡æ¡£ä¸­æ„å»ºè¯æ±‡è¡¨"""
        for doc in documents:
            words = re.findall(r'\b\w+\b', doc.lower())
            for word in words:
                if len(word) > 2:
                    self.word_freq[word] += 1
                    self.vocabulary.add(word)

        min_freq = max(1, len(documents) // 1000)
        self.vocabulary = {word for word, freq in self.word_freq.items() if freq >= min_freq}

        logging.info(f"æ„å»ºè¯æ±‡è¡¨å®Œæˆï¼ŒåŒ…å« {len(self.vocabulary)} ä¸ªè¯æ±‡")

    def levenshtein_distance(self, s1: str, s2: str) -> int:
        """è®¡ç®—ç¼–è¾‘è·ç¦»"""
        if len(s1) < len(s2):
            return self.levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    def get_candidates(self, word: str) -> List[Tuple[str, int]]:
        """è·å–æ‹¼å†™å€™é€‰è¯"""
        if word in self.vocabulary:
            return [(word, 0)]

        candidates = []
        word_lower = word.lower()

        for vocab_word in self.vocabulary:
            if abs(len(vocab_word) - len(word_lower)) > self.max_distance:
                continue

            distance = self.levenshtein_distance(word_lower, vocab_word)
            if distance <= self.max_distance:
                candidates.append((vocab_word, distance))

        candidates.sort(key=lambda x: (x[1], -self.word_freq.get(x[0], 0)))
        return candidates[:5]

    def correct_query(self, query: str) -> Tuple[str, List[str]]:
        """çº æ­£æŸ¥è¯¢è¯­å¥"""
        words = re.findall(r'\b\w+\b', query.lower())
        corrected_words = []
        corrections = []

        for word in words:
            candidates = self.get_candidates(word)
            if candidates:
                best_word, distance = candidates[0]
                corrected_words.append(best_word)
                if distance > 0:
                    corrections.append(f"{word} â†’ {best_word}")
            else:
                corrected_words.append(word)

        corrected_query = ' '.join(corrected_words)
        return corrected_query, corrections


class SynonymExpander:
    """åŒä¹‰è¯æ‰©å±•å™¨"""

    def __init__(self):
        self.synonyms = {
            # ç”µå­äº§å“
            'phone': ['smartphone', 'mobile', 'cellphone', 'iphone', 'android'],
            'smartphone': ['phone', 'mobile', 'cellphone'],
            'laptop': ['notebook', 'computer', 'pc'],
            'computer': ['laptop', 'desktop', 'pc'],
            'tablet': ['ipad', 'pad'],

            # æœè£…
            'shirt': ['tshirt', 'blouse', 'top'],
            'pants': ['trousers', 'jeans'],
            'shoes': ['sneakers', 'boots', 'footwear'],

            # å®¶å±…
            'chair': ['seat', 'stool'],
            'table': ['desk'],
            'lamp': ['light', 'lighting'],

            # ä¹¦ç±
            'book': ['novel', 'textbook', 'ebook'],
            'magazine': ['journal', 'periodical'],

            # é€šç”¨è¯æ±‡
            'good': ['great', 'excellent', 'quality'],
            'cheap': ['affordable', 'budget', 'inexpensive'],
            'expensive': ['premium', 'luxury', 'high-end'],
            'small': ['mini', 'compact', 'portable'],
            'large': ['big', 'huge', 'xl'],
        }

        self.reverse_synonyms = {}
        for word, syns in self.synonyms.items():
            for syn in syns:
                if syn not in self.reverse_synonyms:
                    self.reverse_synonyms[syn] = []
                self.reverse_synonyms[syn].append(word)

    def expand_query(self, query: str, max_expansions: int = 2) -> str:
        """æ‰©å±•æŸ¥è¯¢è¯­å¥"""
        words = re.findall(r'\b\w+\b', query.lower())
        expanded_terms = set(words)

        for word in words:
            if word in self.synonyms:
                expanded_terms.update(self.synonyms[word][:max_expansions])
            if word in self.reverse_synonyms:
                expanded_terms.update(self.reverse_synonyms[word][:max_expansions])

        return ' '.join(expanded_terms)


class HybridSearchEngine:
    """æ··åˆæ£€ç´¢æœç´¢å¼•æ“"""

    def __init__(self, data: pd.DataFrame, semantic_model: str = "all-MiniLM-L6-v2"):
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

        self.data = data.copy()

        # å…³é”®è¯æœç´¢ç»„ä»¶
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None

        # è¯­ä¹‰æœç´¢ç»„ä»¶
        self.semantic_engine = SemanticSearchEngine(semantic_model)

        # æ‹¼å†™çº é”™å’ŒåŒä¹‰è¯æ‰©å±•
        self.spell_corrector = SpellCorrector()
        self.synonym_expander = SynonymExpander()

        # æ··åˆæ£€ç´¢æƒé‡
        self.keyword_weight = 0.6
        self.semantic_weight = 0.4

        self._build_index()

    def _build_index(self):
        """æ„å»ºæ··åˆæœç´¢ç´¢å¼•"""
        self.logger.info("æ„å»ºæ··åˆæœç´¢ç´¢å¼•...")

        # å‡†å¤‡æ–‡æ¡£
        documents = []
        for _, row in self.data.iterrows():
            search_text = row.get('search_text', '')
            if pd.notna(search_text) and search_text.strip():
                documents.append(str(search_text))
            else:
                documents.append("")

        if not documents or all(not doc.strip() for doc in documents):
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æœç´¢æ–‡æ¡£")

        # æ„å»ºæ‹¼å†™çº é”™è¯æ±‡è¡¨
        self.spell_corrector.build_vocabulary(documents)

        # æ„å»ºTF-IDFç´¢å¼•
        self.logger.info("æ„å»ºTF-IDFç´¢å¼•...")
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=config.SEARCH_CONFIG['max_features'],
            stop_words='english',
            lowercase=True,
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.8
        )
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(documents)

        # æ„å»ºè¯­ä¹‰åµŒå…¥ç´¢å¼•
        if self.semantic_engine.model:
            self.semantic_engine.build_embeddings(documents)
        else:
            print("âš ï¸  è¯­ä¹‰æœç´¢ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨çº¯å…³é”®è¯æœç´¢æ¨¡å¼")

        self.logger.info("æ··åˆæœç´¢ç´¢å¼•æ„å»ºå®Œæˆ")

    def search(self, query: str, top_k: int = None, filters: Dict = None,
               enable_spell_correction: bool = True,
               enable_synonym_expansion: bool = True,
               use_hybrid: bool = True,
               keyword_weight: float = 0.6) -> Dict[str, Any]:
        """æ··åˆæ£€ç´¢æœç´¢"""
        if not query or not query.strip():
            return {'results': [], 'query_info': {}}

        if top_k is None:
            top_k = config.SEARCH_CONFIG['default_top_k']

        # æ›´æ–°æƒé‡
        self.keyword_weight = keyword_weight
        self.semantic_weight = 1.0 - keyword_weight

        try:
            original_query = query.strip()
            processed_query = original_query.lower()
            query_info = {
                'original_query': original_query,
                'processed_query': processed_query,
                'corrections': [],
                'expansions': [],
                'search_strategy': 'hybrid' if (use_hybrid and self.semantic_engine.model) else 'keyword_only',
                'keyword_weight': self.keyword_weight,
                'semantic_weight': self.semantic_weight
            }

            # ç¬¬ä¸€æ­¥ï¼šå°è¯•æ··åˆæœç´¢
            if use_hybrid and self.semantic_engine.model:
                results = self._execute_hybrid_search(processed_query, top_k, filters)
            else:
                results = self._execute_keyword_search(processed_query, top_k, filters)
                query_info['search_strategy'] = 'keyword_only'

            # å¦‚æœç»“æœä¸å¤Ÿå¥½ï¼Œå°è¯•æ‹¼å†™çº é”™
            if (enable_spell_correction and
                    (not results or len(results) < top_k // 2 or
                     (results and results[0]['score'] < 0.1))):

                corrected_query, corrections = self.spell_corrector.correct_query(processed_query)
                if corrections:
                    if use_hybrid and self.semantic_engine.model:
                        corrected_results = self._execute_hybrid_search(corrected_query, top_k, filters)
                    else:
                        corrected_results = self._execute_keyword_search(corrected_query, top_k, filters)

                    if corrected_results and (not results or corrected_results[0]['score'] > results[0]['score']):
                        results = corrected_results
                        query_info['processed_query'] = corrected_query
                        query_info['corrections'] = corrections
                        query_info['search_strategy'] += '_spell_corrected'

            # å¦‚æœç»“æœä»ç„¶ä¸å¤Ÿå¥½ï¼Œå°è¯•åŒä¹‰è¯æ‰©å±•
            if (enable_synonym_expansion and
                    (not results or len(results) < top_k // 2 or
                     (results and results[0]['score'] < 0.1))):

                expanded_query = self.synonym_expander.expand_query(query_info['processed_query'])
                if expanded_query != query_info['processed_query']:
                    if use_hybrid and self.semantic_engine.model:
                        expanded_results = self._execute_hybrid_search(expanded_query, top_k, filters)
                    else:
                        expanded_results = self._execute_keyword_search(expanded_query, top_k, filters)

                    if expanded_results and (not results or expanded_results[0]['score'] > results[0]['score'] * 0.8):
                        combined_results = self._merge_results(results, expanded_results, top_k)
                        if len(combined_results) > len(results):
                            results = combined_results
                            query_info['expansions'] = expanded_query.split()
                            query_info['search_strategy'] += '_synonym_expanded'

            return {
                'results': results,
                'query_info': query_info,
                'total_count': len(results)
            }

        except Exception as e:
            self.logger.error(f"æœç´¢å¤±è´¥: {e}")
            return {'results': [], 'query_info': {'error': str(e)}}

    def _execute_hybrid_search(self, query: str, top_k: int, filters: Dict = None) -> List[Dict[str, Any]]:
        """æ‰§è¡Œæ··åˆæ£€ç´¢æœç´¢"""
        if not query.strip():
            return []

        # å…³é”®è¯æœç´¢ç»“æœ
        keyword_results = self._get_keyword_similarities(query)

        # è¯­ä¹‰æœç´¢ç»“æœ
        semantic_results = self._get_semantic_similarities(query)

        # æ··åˆå¾—åˆ†è®¡ç®—
        hybrid_scores = {}
        max_results = min(len(self.data), top_k * 3)

        # å¤„ç†å…³é”®è¯æœç´¢ç»“æœ
        for idx, score in keyword_results[:max_results]:
            hybrid_scores[idx] = score * self.keyword_weight

        # å¤„ç†è¯­ä¹‰æœç´¢ç»“æœ
        for idx, score in semantic_results[:max_results]:
            if idx in hybrid_scores:
                hybrid_scores[idx] += score * self.semantic_weight
            else:
                hybrid_scores[idx] = score * self.semantic_weight

        # æ’åºå¹¶æ„å»ºç»“æœ
        sorted_results = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)

        results = []
        for idx, score in sorted_results[:top_k * 2]:
            if score > config.SEARCH_CONFIG['min_score_threshold']:
                product = self.data.iloc[idx]

                result = {
                    'product_id': int(product.get('product_id', idx)),
                    'title': str(product.get('title', '')),
                    'main_category': str(product.get('main_category', '')),
                    'description': str(product.get('description_clean', '')),
                    'price': self._safe_float(product.get('price_clean')),
                    'average_rating': self._safe_float(product.get('average_rating')),
                    'rating_number': self._safe_int(product.get('rating_number')),
                    'asin': str(product.get('asin', '')),
                    'score': float(score),
                    'search_type': 'hybrid'
                }
                results.append(result)

        # åº”ç”¨è¿‡æ»¤å™¨
        if filters:
            results = self._apply_filters(results, filters)

        return results[:top_k]

    def _execute_keyword_search(self, query: str, top_k: int, filters: Dict = None) -> List[Dict[str, Any]]:
        """æ‰§è¡Œå…³é”®è¯æœç´¢"""
        keyword_results = self._get_keyword_similarities(query)

        results = []
        for idx, score in keyword_results[:top_k * 2]:
            if score > config.SEARCH_CONFIG['min_score_threshold']:
                product = self.data.iloc[idx]

                result = {
                    'product_id': int(product.get('product_id', idx)),
                    'title': str(product.get('title', '')),
                    'main_category': str(product.get('main_category', '')),
                    'description': str(product.get('description_clean', '')),
                    'price': self._safe_float(product.get('price_clean')),
                    'average_rating': self._safe_float(product.get('average_rating')),
                    'rating_number': self._safe_int(product.get('rating_number')),
                    'asin': str(product.get('asin', '')),
                    'score': float(score),
                    'search_type': 'keyword'
                }
                results.append(result)

        if filters:
            results = self._apply_filters(results, filters)

        return results[:top_k]

    def _get_keyword_similarities(self, query: str) -> List[Tuple[int, float]]:
        """è·å–å…³é”®è¯ç›¸ä¼¼åº¦"""
        query_vector = self.tfidf_vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()
        top_indices = np.argsort(similarities)[::-1]

        return [(int(idx), float(similarities[idx])) for idx in top_indices
                if similarities[idx] > 0]

    def _get_semantic_similarities(self, query: str) -> List[Tuple[int, float]]:
        """è·å–è¯­ä¹‰ç›¸ä¼¼åº¦"""
        if not self.semantic_engine.model:
            return []

        return self.semantic_engine.search(query, top_k=len(self.data))

    def _merge_results(self, results1: List[Dict], results2: List[Dict], top_k: int) -> List[Dict]:
        """åˆå¹¶æœç´¢ç»“æœ"""
        seen_ids = set()
        merged = []

        all_results = results1 + results2
        all_results.sort(key=lambda x: x['score'], reverse=True)

        for result in all_results:
            product_id = result['product_id']
            if product_id not in seen_ids:
                seen_ids.add(product_id)
                merged.append(result)
                if len(merged) >= top_k:
                    break

        return merged

    def _safe_float(self, value) -> Optional[float]:
        """å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
        if pd.isna(value) or value is None:
            return None
        try:
            return float(value)
        except:
            return None

    def _safe_int(self, value) -> Optional[int]:
        """å®‰å…¨è½¬æ¢ä¸ºæ•´æ•°"""
        if pd.isna(value) or value is None:
            return None
        try:
            return int(value)
        except:
            return None

    def _apply_filters(self, results: List[Dict], filters: Dict) -> List[Dict]:
        """åº”ç”¨è¿‡æ»¤å™¨"""
        filtered_results = results.copy()

        if 'category' in filters:
            target_category = str(filters['category']).lower()
            filtered_results = [
                r for r in filtered_results
                if target_category in r['main_category'].lower()
            ]

        if 'price_min' in filters:
            price_min = filters['price_min']
            filtered_results = [
                r for r in filtered_results
                if r['price'] is not None and r['price'] >= price_min
            ]

        if 'price_max' in filters:
            price_max = filters['price_max']
            filtered_results = [
                r for r in filtered_results
                if r['price'] is not None and r['price'] <= price_max
            ]

        return filtered_results

    def get_search_capabilities(self) -> Dict[str, bool]:
        """è·å–æœç´¢èƒ½åŠ›ä¿¡æ¯"""
        return {
            'keyword_search': True,
            'semantic_search': self.semantic_engine.model is not None,
            'spell_correction': True,
            'synonym_expansion': True,
            'hybrid_search': self.semantic_engine.model is not None
        }


# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸæœ‰çš„SearchEngineç±»
SearchEngine = HybridSearchEngine